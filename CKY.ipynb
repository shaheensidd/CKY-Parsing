{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_CKY_20030.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA39UDyGZh5n"
      },
      "source": [
        "#importing & downloading required libraries\n",
        "import nltk\n",
        "import copy\n",
        "from random import randrange\n",
        "\n",
        "import os.path\n",
        "from node_of_tree import Node"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7fIwLqypI6Y"
      },
      "source": [
        "#change the version to avoid unicode error\n",
        "import sys\n",
        "if sys.version_info[0] >= 3:\n",
        "    unicode = str"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtC5bn_co4Zk",
        "outputId": "17aa1aa0-06d9-4f6c-87b7-d7f263ea450b"
      },
      "source": [
        "#download the resources\n",
        "nltk.download('large_grammars')"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package large_grammars to /root/nltk_data...\n",
            "[nltk_data]   Package large_grammars is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cplu_UEuVjV",
        "outputId": "a82cd6b8-7b00-43a5-e857-cf84ffcc8c44"
      },
      "source": [
        "#loading the grammar\n",
        "\n",
        "#The ATIS CFG is available in the NLTK data package, together with 98 test sentences.\n",
        "\n",
        "grammar = nltk.data.load(\"grammars/large_grammars/atis.cfg\")\n",
        "grammar"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Grammar with 5517 productions>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Wb3dpK-uDls",
        "outputId": "03ef651a-85bc-4d1c-98cb-c2198bc15ef0"
      },
      "source": [
        "#initialization of the following variables:\n",
        "  ### 'productions_all'-----> dictionary \n",
        "  ### 'sentences_all' ------> lists\n",
        "  ### 'states_all' ---------> lists\n",
        "\n",
        "productions = {}\n",
        "sentences = []\n",
        "states = []\n",
        "\n",
        "\n",
        "\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
        "\n",
        "\n",
        "\n",
        "# 1. -----> From the CFG grammar find the following:\n",
        "            ### productions\n",
        "            ### sentences\n",
        "            ### states\n",
        "\n",
        "for i in range(len(grammar.productions())):\n",
        "    keys = grammar.productions()[i].lhs()\n",
        "    keys = str(keys)\n",
        "    value1 = list(grammar.productions()[i].rhs())\n",
        "    for i in range(len(value1)):\n",
        "        if isinstance(value1[i], (str, unicode)):\n",
        "            value1[i]=\"'\"+value1[i]+\"'\"\n",
        "        else:\n",
        "            value1[i]=str(value1[i])\n",
        "    if keys not in states:\n",
        "        states.append(keys)    \n",
        "        productions[keys]=[value1]\n",
        "    else:\n",
        "        if value1 not in productions[keys]:\n",
        "            productions[keys].append(value1)\n",
        "\n",
        "\n",
        "\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
        "\n",
        "#rules for binarization (CNF form)\n",
        "\n",
        "#Final rules like A -> a\n",
        "#Rules like A -> BC will go to CNF\n",
        "#no empty RHS\n",
        "\n",
        "\n",
        "# 2. -------> >2 to 2(Binarization)\n",
        "\n",
        "def binarization():\n",
        "    #variable for int_counting\n",
        "    int_count =0\n",
        "\n",
        "    #Make a copy of 'productions', as it's length will change after Binarization\n",
        "    temp_prod_dict = copy.deepcopy(productions)\n",
        "\n",
        "    for keys in temp_prod_dict:\n",
        "        values = temp_prod_dict[keys]\n",
        "\n",
        "        for i in range(len(values)):\n",
        "\n",
        "            # checking for >2 case rule violation\n",
        "            if len(values[i]) > 2:\n",
        "            \n",
        "                for j in range(0, len(values[i]) - 2):\n",
        "\n",
        "                    # replacing\n",
        "                    if j==0:\n",
        "                        new_list = []\n",
        "                        new_list.append(productions[keys][i][0])  #appending the new_list\n",
        "\n",
        "                        letter = 'A' + str(int_count)\n",
        "                        new_list.append(letter)    #appending the new_list\n",
        "\n",
        "                        int_count = int_count + 1  #incrementing the count variable\n",
        "                        productions[keys][i] = new_list   #reassigning \n",
        "                    \n",
        "                    # adding the new productions\n",
        "                    else:\n",
        "                        new_list = []\n",
        "                        new_list.append(values[i][j])   #appending the new_list\n",
        "\n",
        "                        letter = 'A' + str(int_count)\n",
        "                        new_list.append(letter)    #appending the new_list\n",
        "\n",
        "                        int_count = int_count + 1  #incrementing the count variable\n",
        "                        productions.setdefault(key_new, []).append(new_list)  #appending\n",
        "                    \n",
        "                    states.append(letter)   #appending\n",
        "                    \n",
        "                    # saving letter copy\n",
        "                    # will be useing in the next rule\n",
        "                    key_new = copy.deepcopy(letter)\n",
        "                    \n",
        "                # the last two letters will remain the same\n",
        "                productions[key_new] = []\n",
        "                productions[key_new].append(values[i][-2:])\n",
        "\n",
        "\n",
        "\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
        "\n",
        "\n",
        "\n",
        "# 3 -------> Removing the unary (=1) productions (A -> B)\n",
        "\n",
        "def remove_unary():\n",
        "\n",
        "    #pointer variable\n",
        "    flag = 1\n",
        "\n",
        "    #variable for counting\n",
        "    int_count = 0\n",
        "    while flag:\n",
        "\n",
        "        #increment the counting variable\n",
        "        int_count = int_count + 1   \n",
        "\n",
        "        #to check for no variable\n",
        "        flag = 0\n",
        "\n",
        "        #Make a copy of 'productions', as it's length will change after elimination\n",
        "        temp_prod_dict = copy.deepcopy(productions)\n",
        "\n",
        "        for keys in temp_prod_dict:\n",
        "\n",
        "            values = temp_prod_dict[keys]\n",
        "\n",
        "            for i in range(len(values)):\n",
        "\n",
        "                # Check for the rule violation\n",
        "                if len(values[i])==1 and values[i][0] in states and values[i][0]!= keys:\n",
        "                    new_list = copy.deepcopy(values[i][0])    #copy values to list\n",
        "                    productions[keys].remove(values[i])\n",
        "                    value_new = productions[new_list]       #reassigning values after removing duplicates\n",
        "\n",
        "                    for j in range(len(value_new)):\n",
        "                        productions[keys].append(value_new[j])\n",
        "        \n",
        "        for keys in productions:\n",
        "            values = productions[keys]\n",
        "\n",
        "            for k in range(len(values)):\n",
        "\n",
        "                # Checking for the rule violation\n",
        "                if len(values[k])==1 and values[k][0] in states and values[k][0]!= productions[values[k][0]]:\n",
        "                    flag = 1\n",
        "                    break\n",
        "\n",
        "            if flag:\n",
        "                break\n",
        "\n",
        "\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
        "\n",
        "\n",
        "\n",
        "# Print the list 'productions' by passing different rules and writing it into a file each time\n",
        "def print_rules(a_rule_num):\n",
        "   \n",
        "    with open('/content/'+ a_rule_num +'.txt', 'w') as f:   #opening a new file to write\n",
        "\n",
        "        #counting variable\n",
        "        int_count = 0\n",
        "\n",
        "        for keys in productions:\n",
        "            values = productions[keys]\n",
        "\n",
        "            for i in range(len(values)):\n",
        "\n",
        "                new_string = \"\"\n",
        "                int_count = int_count + 1\n",
        "\n",
        "                new_string += keys + '->' + str(values[i]) + '\\n'\n",
        "                f.write(new_string)    #writing to the new file \n",
        "\n",
        "        print(int_count)\n",
        "\n",
        "\n",
        "\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
        "\n",
        "\n",
        "\n",
        "# removing the duplicate productions\n",
        "\n",
        "def remove():\n",
        "    \n",
        "    #Make a copy of 'productions', as it's length will change after elimination\n",
        "    temp_prod_dict = copy.deepcopy(productions)\n",
        "\n",
        "    for keys in temp_prod_dict:\n",
        "\n",
        "        values = temp_prod_dict[keys]\n",
        "        new_list = []\n",
        "\n",
        "        for i in values:\n",
        "\n",
        "            if i in new_list:\n",
        "                productions[keys].remove(i)\n",
        "\n",
        "            else:\n",
        "                new_list.append(i)\n",
        "\n",
        "\n",
        "\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
        "\n",
        "\n",
        "\n",
        "#Parse a test sentence using CYK algorithm   \n",
        "\n",
        "def CKYparser(test_sentence):\n",
        "\n",
        "    test_sentence_length = len(test_sentence)\n",
        "\n",
        "    for i in range(len(test_sentence)):\n",
        "        test_sentence[i] = \"'\" + test_sentence[i] + \"'\"\n",
        "\n",
        "    #create a matrix for storage\n",
        "    matrix_chart = [[[] for i in range(test_sentence_length)] for j in range(test_sentence_length)]\n",
        "\n",
        "    #create a trace pointer for back tracing the matrix\n",
        "    back_trace_pointer = [[[] for i in range(test_sentence_length)] for j in range(test_sentence_length)]\n",
        "\n",
        "    for j in range(1, test_sentence_length):\n",
        "\n",
        "        for keys in productions:\n",
        "            val = productions[keys]\n",
        "\n",
        "            for i in val:\n",
        "                if len(i) == 1  and  test_sentence[j-1] == i[0]:\n",
        "                    matrix_chart[j-1][j].append(keys)\n",
        "                    back_trace_pointer[j - 1][j].append(Node(keys, None, None, test_sentence[j - 1]))\n",
        "        \n",
        "        #traversing back using back_trace_pointer\n",
        "        for i in reversed(range(0, j-1)):\n",
        "\n",
        "            for k in range(i+1, j):\n",
        "\n",
        "                for keys in productions:\n",
        "                    values = productions[keys]\n",
        "\n",
        "                    for v in range(len(values)):\n",
        "                        if len(values[v]) == 2:\n",
        "                            first_val = values[v][0]\n",
        "                            second_val = values[v][1]\n",
        "\n",
        "                            if first_val in matrix_chart[i][k] and second_val in matrix_chart[k][j]:\n",
        "                                matrix_chart[i][j].append(keys)\n",
        "\n",
        "                                for p in back_trace_pointer[i][k]:\n",
        "\n",
        "                                    for q in back_trace_pointer[k][j]:\n",
        "\n",
        "                                        if p.root == first_val and q.root == second_val:\n",
        "                                            back_trace_pointer[i][j].append(Node(keys, p, q, None))\n",
        "\n",
        "    #return output                                       \n",
        "    return back_trace_pointer[0][test_sentence_length-1]\n",
        "\n",
        "\n",
        "\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
        "\n",
        "\n",
        "#Returns the possible trees \n",
        "\n",
        "def tree(root, inter_node):\n",
        "\n",
        "    #using the class and it's objects\n",
        "    if root.status:\n",
        "        return '(' + root.root + ' ' + root.terminal + ')'\n",
        "    \n",
        "    #incrementing the LHS length\n",
        "    l = inter_node + 2 + len(root.left.root) \n",
        "\n",
        "    #incrementing the RHS length\n",
        "    r = inter_node + 2 + len(root.right.root) \n",
        "\n",
        "    #make left\n",
        "    left = tree(root.left, l)\n",
        "\n",
        "    #make right\n",
        "    right = tree(root.right, r)\n",
        "\n",
        "    return '(' + root.root + ' ' + left + '\\n' + ' '*inter_node + right + ')'\n",
        "\n",
        "\n",
        "\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
        "\n",
        "\n",
        "\n",
        "#generate a integer random number between 0-97 -----> to find the test sentence\n",
        "random_number = randrange(98)\n",
        "\n",
        "print(\"Random sentence no. selected is \", random_number + 1)\n",
        "print('\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n",
        "\n",
        "\n",
        "\n",
        "#main function to find parse trees by calling other functions within itself\n",
        "def main():\n",
        "\n",
        "    #about cfg dataset\n",
        "    print('\\nTotal no. of Initial productions rules from ATIS dataset')\n",
        "    print_rules('File 1.0')\n",
        "\n",
        "    #binarization of the productions values\n",
        "    print('\\nTotal no. of Rules after the Binarization and removal of duplicates')\n",
        "    binarization()\n",
        "    #remove()\n",
        "    print_rules('File 2.0')\n",
        "    \n",
        "    print('\\nTotal no. of Final CNF form rules after unary productions removal')\n",
        "    remove_unary()\n",
        "    #remove()\n",
        "    print_rules('File 3.0')\n",
        "\n",
        "    #------------------------------------------------------------------------------------\n",
        "    #####Cockeâ€“Kasami-Younger Parser/CKY parser###################################\n",
        "    #print the trees using CKY parser function, grammar and test sentence\n",
        "\n",
        "    #load the test sentences\n",
        "    s = nltk.data.load(\"grammars/large_grammars/atis_sentences.txt\")\n",
        "\n",
        "    #extracting the test sentences\n",
        "    s = nltk.parse.util.extract_test_sentences(s)\n",
        "    \n",
        "    for sentence in s:\n",
        "        sentences.append(sentence[0])\n",
        "  \n",
        "    #storing the random integer  \n",
        "    sent_int_count = random_number\n",
        "\n",
        "    #finding the test_sentence at depending upon the random integer \n",
        "    test_sentence = sentences[random_number]\n",
        "\n",
        "    #call CKYparser and store in a reverse pointer\n",
        "    back_trace_pointer = CKYparser(test_sentence)\n",
        "\n",
        "    start = str(grammar.start())\n",
        "\n",
        "    #pointer \n",
        "    flag = 0\n",
        "\n",
        "    #counting variable\n",
        "    int_count = 0\n",
        "    \n",
        "    sent_int_count = sent_int_count + 1\n",
        "\n",
        "    #using the class Node in file node.py to generate and print parse trees\n",
        "    for node in back_trace_pointer:\n",
        "        if node.root == start:\n",
        "            int_count = int_count + 1\n",
        "            print(\"\\n\\n\\nParse trees for sentence no. \" , sent_int_count, \":\\n\")\n",
        "            print(tree(node, 3))\n",
        "            print(\"\\n**-----**---------**---------**-----------**---------**---------**----------**---------**----------**---------**\\n\")\n",
        "            \n",
        "            flag = 1\n",
        "\n",
        "    #check flag if test sentence is not found\n",
        "    if flag==0:\n",
        "            print('\\n\\n\\n\"\"\"\"\"\"\"\"\"\"\"\"\"Sentence NOT FOUND\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\n')\n",
        "\n",
        "    #reset sentence to blank\n",
        "    sentence = \"\"\n",
        "\n",
        "    #appending the blank sentence\n",
        "    for i in test_sentence:\n",
        "        sentence = sentence + i\n",
        "\n",
        "    #printing the no. of trees generater by the CKY parser    \n",
        "    print(\"\\nSentence no.\" , sent_int_count, 'has', str(int_count), 'no. of parse trees')\n",
        "                    \n",
        "\n",
        "#calling the main function for execution\n",
        "main()"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random sentence no. selected is  55\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Total no. of Initial productions rules from ATIS dataset\n",
            "5517\n",
            "\n",
            "Total no. of Rules after the Binarization and removal of duplicates\n",
            "13500\n",
            "\n",
            "Total no. of Final CNF form rules after unary productions removal\n",
            "20344\n",
            "\n",
            "\n",
            "\n",
            "Parse trees for sentence no.  55 :\n",
            "\n",
            "(SIGMA (NP_NN (ADJ_WPS 'what')\n",
            "          (A5097 (NP_NNS 'flights')\n",
            "                 (NOUN_NN 'leave')))\n",
            "   (A6148 (NOUN_NP 'boston')\n",
            "          (PP_NP (PREP_IN 'to')\n",
            "                 (NOUN_NP 'pittsburgh'))))\n",
            "\n",
            "**-----**---------**---------**-----------**---------**---------**----------**---------**----------**---------**\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Parse trees for sentence no.  55 :\n",
            "\n",
            "(SIGMA (NP_NN (NP_NNS (ADJ_WPS 'what')\n",
            "                  (NOUN_NNS 'flights'))\n",
            "          (NOUN_NN 'leave'))\n",
            "   (A6148 (NOUN_NP 'boston')\n",
            "          (PP_NP (PREP_IN 'to')\n",
            "                 (NOUN_NP 'pittsburgh'))))\n",
            "\n",
            "**-----**---------**---------**-----------**---------**---------**----------**---------**----------**---------**\n",
            "\n",
            "\n",
            "Sentence no. 55 has 2 no. of parse trees\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}